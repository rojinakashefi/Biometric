{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/logo.png)\n",
    "# Biometrics System Concepts\n",
    "## Assignment 1: Evaluating performance of biometric systems\n",
    "<b>Name</b>: Marty McFly |\n",
    "<b>Student-nr</b>: C25 |\n",
    "<b>Date</b>: June 12, 2015\n",
    "---\n",
    "\n",
    "In this assignment we focus on evaluating the performance of any biometric system in a verification and identification setting. \n",
    "\n",
    "Before you get started you should be able to properly distinguish verification from identification and know the corresponding validation procedures. If this is not the case for you yet, **review the lecture notes!** We will give a short recap to refresh your memory: \n",
    "* **VERIFICATION** (a.k.a. authentication): Authenticating a claimed identity (is this person who he/she claims to be?).\n",
    "\n",
    "* **IDENTIFICATION**: Associate a particular individual with an identity (who is this unidentified individual?).\n",
    "\n",
    "This document is structured as follows:\n",
    "\n",
    "- [I. Reading the data](#I.-Reading-the-data)\n",
    "- [II. Validation of verification system](#II.-Validation-of-verification-system)\n",
    "- [III. Validation of identification system](#III.-Validation-of-identification-system)\n",
    "- [IV. Assignment Instructions](#IV.-Assignment-Instructions)\n",
    "\n",
    "\n",
    "Code examples will be provided below. You can and are invited to adapt these at your will (different parameter settings, different choices of alogorithmic components). The code examples in this assignment are just sekeleton code,  **adapt where needed! And try to keep things structured!** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Install and import the required python packages to run this notebook. Feel free to add more packages whenever needed.\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# the following meta-command is required to get plots displayed in notebooks\n",
    "%matplotlib inline\n",
    "\n",
    "# package for data analysis with fast and flexible data structures\n",
    "import pandas as pd\n",
    "\n",
    "# package to show a nice graphical progress-bar for lengthy calculations\n",
    "# docu and installation on https://tqdm.github.io\n",
    "# if you have difficulties installing this package: \n",
    "# - make sure your jupyter lab is up to date\n",
    "# - https://github.com/tqdm/tqdm/issues/394#issuecomment-384743637\n",
    "# - consider just leaving it out (just remove the 'tqdm_notebook' in the code)\n",
    "from tqdm.notebook import tqdm as tqdm_notebook\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Tip:</b> \n",
    "Many operations have already been implemented in <a href=\"https://docs.scipy.org/doc/numpy/index.html\">SciPy</a>, feel free to use them or any other unless explicitely stated not to in the assignment.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Reading the data\n",
    "In this assignment we focus on the validation of a pre-exising biometric system. We will make use of synthetic similarity scores of four different biometric systems. This bypasses all steps of preprocessing, feature extraction and matching and allows us to concentrate on the score evaluation procedures. The scores are the result of comparing an enrolled user's image with the score of the same (genuine scores) or another user (impostor scores). The data can be found in the .csv files in the `./data/` subfolder. \n",
    "\n",
    "In this code example we will compare the performance of four biometric systems. These systems are referred to as `system1`, `system2`, `system3` and `system4` respectively in both code and text. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Tip:</b> \n",
    "For each question, make sure to <b> compare the results for the different systems</b>!!! Mention which system performs the best/worst and why.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Loading the similarity matrix and reading the genuine and impostor scores\n",
    "We provide you with the code to read the scores from the files (normally you can leave this code as-is).\n",
    "\n",
    "The code consits of 2 steps that are executed for all the systems: \n",
    "1. Scores are converted to similarity matrices. To simplify the task we have provided you with the similarity matrices. You need to load the similarity matrices with the genuine scores on the diagonal and the impostor scores on the off-diagonal elements.  \n",
    "2. Convert the original scores to a linear list of scores with associated labels of genuine or impostor (simplifies use of [scikit-learn](https://scikit-learn.org/stable/index.html))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "[[ 8.81217268e+00 -2.75825091e-01 -4.37851532e+00 ...  1.00965251e+00\n",
      "  -1.65898630e+00  1.16527521e+00]\n",
      " [ 2.49628607e+00  7.69412179e+00  8.81129913e-01 ... -1.38940286e+00\n",
      "  -1.12078184e+00 -2.71187305e-01]\n",
      " [-2.52004120e+00 -2.34191894e+00  7.73591412e+00 ... -1.43071748e+00\n",
      "  -2.26625492e-01 -1.72862352e+00]\n",
      " ...\n",
      " [-8.17260699e-01 -1.04769038e+00 -5.99610925e-02 ...  7.96518773e+00\n",
      "   2.29049260e+00  4.37915460e-01]\n",
      " [ 1.49096040e+00 -2.05430723e+00  9.84164534e-01 ... -1.68939222e+00\n",
      "   8.17693521e+00 -3.12281078e-01]\n",
      " [ 1.97809280e+00  8.76217475e-03  3.02965676e+00 ...  2.13487178e+00\n",
      "   5.64116425e-01  7.90652249e+00]]\n"
     ]
    }
   ],
   "source": [
    "# STEP 1: loading the similarity matrices for the four systems\n",
    "with open('./data/system1_similarity_matrix.csv','rb') as f:\n",
    "    system1_similarity_matrix = pickle.load(f)\n",
    "with open('./data/system2_similarity_matrix.csv','rb') as f:\n",
    "    system2_similarity_matrix = pickle.load(f)\n",
    "with open('./data/system3_similarity_matrix.csv','rb') as f:\n",
    "    system3_similarity_matrix = pickle.load(f)\n",
    "with open('./data/system4_similarity_matrix.csv','rb') as f:\n",
    "    system4_similarity_matrix = pickle.load(f)\n",
    "print(type(system1_similarity_matrix))\n",
    "print(system1_similarity_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# STEP 2: convert to genuine and impostor scores, the *_genuine_id provides a mask for the genuine scores\n",
    "def sim2scores(similarity_matrix):\n",
    "    # use .values to access as numpy array\n",
    "    np_similarity_matrix = similarity_matrix\n",
    "\n",
    "    # grab elements on the diagonal\n",
    "    genuine_scores = np.diag(np_similarity_matrix)\n",
    "\n",
    "    # mask elements that are on the diagonal, retain non-diagonal elements\n",
    "    imposter_scores =  np_similarity_matrix[~np.eye(np_similarity_matrix.shape[0],dtype=bool)]\n",
    "\n",
    "    # store in one single list of scores as required for the classification validation procedures\n",
    "    scores = np.append(np.array(genuine_scores), np.array(imposter_scores))\n",
    "\n",
    "    # normalize to [0,1] range, 0 corresponding to minimal similarity\n",
    "    scores = (scores - scores.min())/(scores.max()-scores.min())\n",
    "\n",
    "    ## add the genuine and imposter labels\n",
    "\n",
    "    # tag genuine combinations as label 1 and imposter combinations as 0\n",
    "\n",
    "    genuine_id = np.zeros_like(scores)\n",
    "    genuine_id[0:genuine_scores.shape[0]] = 1\n",
    "\n",
    "    return(genuine_id, scores)\n",
    "\n",
    "\n",
    "system1_genuine_id, system1_scores = sim2scores(system1_similarity_matrix)\n",
    "system2_genuine_id, system2_scores = sim2scores(system2_similarity_matrix)\n",
    "system3_genuine_id, system3_scores = sim2scores(system3_similarity_matrix)\n",
    "system4_genuine_id, system4_scores = sim2scores(system4_similarity_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Tip:</b> \n",
    "In this assignment we focus only on evaluating biometric systems. It is in your best interest to write your code such that it can easily be reused in the upcoming assignments, where you will have to develop entire biometric system pipelines.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Validation of verification system\n",
    "\n",
    "[1]: <https://link.springer.com/book/10.1007/978-0-387-77326-1> ('Introduction to Biometrics' by AK Jain et al)\n",
    "[2]: <https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-classification-in-python/> (How and When to Use ROC Curves and Precision-Recall Curves for Classification in Python)\n",
    "\n",
    "The performance of a verification scenario can be expressed in a number of ways (see [Jain et al.][1] section 1.4.1.1 for more information). In essence one has a binary classification problem: is it the claimed identity or not? \n",
    "\n",
    "We denote our classes as:\n",
    "<ol start=\"0\">\n",
    "  <li>Impostor (False),</li>\n",
    "  <li>Genuine (True).</li>\n",
    "</ol>\n",
    "\n",
    "Furthermore, we represent the set of scores as s, the imposter event as $I$ and a genuine event as $G$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Evaluation using FMR, FRR, ROC and Precision/Recall curves\n",
    "\n",
    "[1]: <https://link.springer.com/book/10.1007/978-0-387-77326-1> ('Introduction to Biometrics' by AK Jain et al)\n",
    "[2]: <https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-classification-in-python/> (How and When to Use ROC Curves and Precision-Recall Curves for Classification in Python)\n",
    "\n",
    "\n",
    "\n",
    "#### 1.1 Genuine and impostor score distributions\n",
    "Given the genuine and impostor scores (from section I), we can plot the imposter $p(s | I)$ and genuine $p(s | G)$ distribution to gain some first insights in the system. The result should look something like this:\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"img/ScoreDistributions.png\" width=\"250\" height=\"auto\"/>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Q1: </b> Score distributions\n",
    "<ul>\n",
    "  <li>Plot the genuine and impostor score distributions in a single plot for the four systems.</li>\n",
    "  <li>Do you need to normalize the distributions? Why (not)?</li>\n",
    "  <li>Describe qualitatively this combined plot. Which system performs the best based on this plot?</li>\n",
    "  <li>Which system corresponds to which of the following descriptions?</li>\n",
    "  <ol type = 'A'>\n",
    "    <li> small inter-user distance, small intra-user variation </li>\n",
    "    <li> small inter-user similarity, small intra-user variation </li>\n",
    "    <li> large inter-user similarity, large intra-user variation </li>\n",
    "    <li> large inter-user distance, large intra-user variation </li>\n",
    "  </ol>\n",
    "</ul>  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Plot the genuine and imposter score distributions.\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. FMR, FRR and Receiver Operating Characteristic (ROC) curve\n",
    "\n",
    "The 'false accept' and 'false reject' regions in the illustration above are quantified using the False Match/Acceptance Rates (FMR/FAR) and False Non-Match/Rejections Rates (FNMR/FRR). The FMR and FNMR can easily be computed given the previously calculated probability distributions and a threshold value $\\eta$. Practically this boils down to a counting problem, having $\\mathcal{I}$ the indicator function (return 1 if x is true, else 0) we can compute:\n",
    "$$\n",
    "FMR(\\eta) = p(s \\geq \\eta | I) \\approx \\frac{1}{|I|} \\sum_{s \\in I} \\mathcal{I}(s \\geq \\eta),\n",
    "$$\n",
    "$$\n",
    "FRR(\\eta) = p(s < \\eta | G) \\approx \\frac{1}{|G|} \\sum_{s \\in G} \\mathcal{I}(s < \\eta).\n",
    "$$\n",
    "\n",
    "We can also compute the Genuine Acceptance Rate (GAR)/True Match Rate (TMR) as:\n",
    "$$\n",
    "GAR(\\eta) = p(s \\geq \\eta | G) = 1 - FRR(\\eta).\n",
    "$$\n",
    "\n",
    "Note that choosing a threshold is always a tradeof between FMR and FNMR. <br>\n",
    "<img src=\"img/FAR_FRR.png\" width=\"300\" height=\"auto\" align=\"center\"/>\n",
    "\n",
    "To observe the impact of the threshold value, one often plots a [Receiver Operating Characteristic (ROC)](https://en.wikipedia.org/wiki/Receiver_operating_characteristic) curve. Usually in these curves the GAR/TMR is plotted against the FMR for different decision threshold values $\\eta$. For those familiar with binary classification systems outside of the biometrics literature; the GAR/TMR is often referred to as the True Positive Rate (TPR), sensitivity or recall and FMR is also known as the False Positive Rate (FPR) or the False Accept Rate (FAR). \n",
    "\n",
    "<img src=\"img/ROC.png\" width=\"700\" height=\"auto\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Q2: </b> ROC Curves\n",
    "<ul>\n",
    "    <li>Calculate FPR, TPR from the matching scores.</li>\n",
    "    <li>Plot FAR and FRR as a function of the decision threshold.</li>\n",
    "    <li>Plot the ROC curve for the four systems <b> in a single plot </b>. Plot for linear and logarithmic scale if needed. What do you observe?</li>\n",
    "    <li>Plot the Detection Error Trade-off (DET) curve. How does it compare to ROC?</li>\n",
    "</ul>  \n",
    "</div>\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Tip:</b> \n",
    "We highly recommend you use the <a href=\"https://scikit-learn.org/stable/index.html\">scikit-learn</a> package, it contains standard and advanced routines for machine learning, including classificaton and validation algorithms.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Calculate FPR, TPR from the matching scores.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Plot FAR and FRR as a function of the decision threshold.\"\"\"\n",
    "# FAR <=> FPR\n",
    "# FRR <=> FNR <=> FN/P <=> 1-TPR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"plot the ROC curve (TPR against the FPR for different threshold values)\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"plot the DET curve (FRR (=1-tpr) against the FAR for different threshold values)\"\"\"\n",
    "\n",
    "# FAR <=> FPR\n",
    "# FRR <=> FNR <=> FN/P <=> 1-TPR\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. F1 and accuracy as metrics\n",
    "\n",
    "While biometric systems are, traditionally, evaluated using FMR and FRR and ROC/DET curves, we can also have a look at traditional [classification metrics](https://scikit-learn.org/stable/modules/model_evaluation.html#classification-metrics) such as classification accuracy (or error) and F1 measure. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Q3: </b> Classification Metrics\n",
    "<ul>\n",
    "    <li>Plot F1 and accuracy as a function of the decision thresholds on the similarity score for the four systems<b> in a single plot </b>.</li>\n",
    "    <li>Calculate the threshold and accuracy for which F1 is maximal. Is it an interesting operating point?</li>\n",
    "    <li>Calculate the threshold and F1 for which accuracy is maximal. Is it an interesting operating point? Is there a difference with the results of the previous question and why?</li>\n",
    "    <li>Is accuracy a good performance metric in this case?</li>\n",
    "</ul>  \n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Tip:</b> \n",
    "We highly recommend you use the <a href=\"https://scikit-learn.org/stable/modules/model_evaluation.html#classification-metrics\">scikit-learn classification metrics</a> to assist.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"Plot F1 and accuracy as a function of the decision thresholds on the similarity score.\"\"\"\n",
    "# Hint: evaluating for ± 50 threshold values should suffice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"Calculate the threshold for which F1 is maximal.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Do the same for the accuracy\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. AUC and EER as summary measures\n",
    "\n",
    "The overall performance (over all threshold settings) is typically expressed through:\n",
    "\n",
    "* The Area Under the Curve (AUC) (with TPR((y-axis) vs FPR (x-axis))\n",
    "\n",
    "<img src=\"img/AUC.png\" width=\"250\" height=\"auto\"/>\n",
    "\n",
    "The AUC can be used to compare different systems. The larger this number, the better.\n",
    "However, since it is a summary measure, always inspect the full ROC curve to make decisions about performance given operating conditions (in wich FRR, FAR regime to work e.g.).\n",
    "\n",
    "* The Equal Error Rate (EER), which is the point on the ROC-curve where FAR(FMR) equals FRR (1-TAR). A lower EER value indicates better performance. \n",
    "\n",
    "<img src=\"img/EER.png\" width=\"300\" height=\"300\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Q4: </b> AUC, EER and alternatives\n",
    "<ul>\n",
    "    <li>Calculate ROC AUC. Is this a good metric? What does it reveal about the system? </li>\n",
    "    <li>Calculate (by approximation) the EER and plot it on the FAR-FRR curve. Is this a good peration point?</li>\n",
    "    <li>Calculate the decision threshold for which the sum of FRR and FAR is minimal. Is this point similar to the total classification error?</li> \n",
    "    <li>Discuss the importance of the FRR and the FAR if you want a very secure versus a very convenient system. </li> \n",
    "</ul>  \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Calculate the ROC AUC.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Calculate (by approximation) the EER and plot it on the FAR-FRR curve.\"\"\"\n",
    "# hints:\n",
    "#  - avoid using a library that directly computes the EER for this assignment \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Calculate the decision threshold for which the sum of FRR and FAR is minimal.\"\"\"\n",
    "# hint: same as above\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Evaluation using Precision and Recall\n",
    "\n",
    "[1]: <https://link.springer.com/book/10.1007/978-0-387-77326-1> ('Introduction to Biometrics' by AK Jain et al)\n",
    "[2]: <https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-classification-in-python/> (How and When to Use ROC Curves and Precision-Recall Curves for Classification in Python)\n",
    "[3]: <https://en.wikipedia.org/w/index.php?title=Information_retrieval&oldid=793358396#Average_precision> (Average precision)\n",
    "\n",
    "In a general binary classification setting, one also often presents Precision-Recall curves. PR-curves are sometimes summarized using the [average precision scores][3]. How and when to use ROC or PRC is discussed [here][2]. A more general discussion of these measures is provided [here](https://towardsdatascience.com/beyond-accuracy-precision-and-recall-3da06bea9f6c).  \n",
    "\n",
    "Scikit-Learn provides routines for calculating these curves and numbers as demonstrated in the code below from this [link][2], it also provides an implementation of the [average precision scores](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.average_precision_score.html#sklearn.metrics.average_precision_score).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "<b>Q5: </b> Precision-Recall curves and related summary measures\n",
    "<ul>\n",
    "    <li>Calculate and plot the Precision-Recall curve for these systems. What does it reveal about the performance of the systems?</li>\n",
    "    <li>Does the ROC curve or the PRC curve make more sense to evaluate these four systems? (Tip: read the webpages mentioned above). </li>\n",
    "    <li>Calculate the Area Under the PR-curve. Discuss.</li>\n",
    "    <li>Calculate the average precision scores. Discuss its value.</li> \n",
    "</ul>  \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Calculate and plot the Precision-Recall curve for the systems\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Calculate the Area Under the PR-curve.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Calculate the average precision scores\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Validation of identification system "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation using CMC curves\n",
    "In an identification scenario one has a 1-to-many or multi-class classification problem. The performance of such a system is typically measured by the Cumulative Match Characteristic (CMC) curve. This curve plots the experimental probability that a correct identification is returned within the top-x (x=1, ..., N) ranked matching scores. \n",
    "\n",
    "<img src=\"img/CMC.jpg\" width=\"500\" height=\"auto\"/>\n",
    "\n",
    "[Bolle et al.](https://ieeexplore.ieee.org/document/1544394) show that:\n",
    "> the CMC is also related to the FAR and FRR of a 1:1 matcher, i.e., the matcher that is used to rank the candidates by sorting the scores. This has as a consequence that when a 1:1 matcher is used for identification, that is, for sorting match scores from high to low, the CMC does not offer any additional information beyond the FAR and FRR curves. The CMC is just another way of displaying the data and can be computed from the FAR and FRR.\n",
    "\n",
    "This paper is not mandatory but those interested can have a look at it.\n",
    "\n",
    "CMC curves can easily be generated once you have the ranked matching scores for every test sample. In our example we can easily calculate it from the similarity matrix (note that in this very particular case we have only one genuine pair per test sample). \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Q6: </b> CMC curves\n",
    "<ul>\n",
    "    <li>Calculate the Cumulative Matching Characteristic curve (implement this yourself)</li>\n",
    "    <li>Compute the Rank-1 Recognition Rate.</li>\n",
    "</ul>  \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Calculate the Cumulative Matching Characteristic curve.\"\"\"\n",
    "# Hint: don't use a library\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"compute the Rank-1 Recognition Rate.\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Q7: </b> Evaluate different biometric systems\n",
    "<ul>\n",
    "    <li>Propose a new metric not mentioned in this task to evaluate these four systems. This can be something you come up with yourself by adjusting/combining some of the existing metrics in this task or something you find online. Explain in what way your new method is more/less suited to evaluate these systems </li>\n",
    "</ul>  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Assignment Instructions\n",
    "For this assignment you have to submit a report (.pdf) and the implementation of this notebook (.ipynb) to toledo. The report should be between 5-8 pages (more pages $\\nRightarrow$ higher score) and should be structured around the posed questions (Q1,...). The text should demonstrate your understanding of the material and, depending on the question, clearly introduce the context, technique, your expectation and interpretation of the results. Do not limit yourselves to just answering the questions. Make sure to compare the performance of the four biometric systems in each question. **The report should be self contained, the notebook functions as supplementary material only! Make sure to add all relevant figures and results in the text!**\n",
    "\n",
    "Your submission should be a .zip file with the name formatted as Assignment1_[Your student number].zip. You should replace [Your student number] with your r-number, with the r. \n",
    "\n",
    "*Note: Make sure you include all the files required to run the notebooks on submission.* <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
