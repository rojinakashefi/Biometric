{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![KU Leuven](./img/logo.png)\n",
    "# Biometrics System Concepts\n",
    "## Assignment 2: Fingerprint and Iris based Identification\n",
    "\n",
    "**Name:** Joe Pesci |\n",
    "**Student Nr:** KD6-3.7 |\n",
    "**Date:** February 10, 1942\n",
    "---\n",
    "\n",
    "Implement and test a keypoint-based fingerprint and iris recognition system and fuse the two systems together.\n",
    "\n",
    "A high-level description is provided with links to or hints of code snippets and libraries that you can reuse/adapt at your will (with proper referencing!).\n",
    "\n",
    "This document is structured as below:  \n",
    "* [I. Setting](#I.-Setting)\n",
    "* [II. Fingerprint Recognition](#II.-Fingerprint-Recognition)\n",
    "  1. Reading data\n",
    "  2. Baseline model\n",
    "    1. Choosing a similarity metric\n",
    "    2. Constructing the similarity table\n",
    "  3. Minutiae-Based MatchingÂ¶\n",
    "    1. Image Enhancement\n",
    "    2. Minutiae Detection\n",
    "    3. Global Matching and Image Alignment\n",
    "    4. Global Similarity Metric\n",
    "    5. Validation\n",
    "* [III. Iris Recognition](#III.-Iris-recognition)\n",
    "  1. Reading data\n",
    "  2. Biometric Iris Recogntion System\n",
    "    1. Image enhancement\n",
    "    2. Triplet Loss Encoder \n",
    "    3. Validation\n",
    "* [IV. Multimodal System](#IV-Multimodal-System)\n",
    "  1. Score fusion\n",
    "  2. Solve the murder case  \n",
    "* [V. Assignment Instructions](#V.-Assignment-Instructions)\n",
    "\n",
    "\n",
    "Code examples will be provided below. You can and are invited to adapt these at your will (different parameter settings, different choices of alogorithmic components, add external python files, ...). Try to keep things structured!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to plot figures inline\n",
    "%matplotlib inline\n",
    "\n",
    "# OpenCV package\n",
    "import cv2\n",
    "# Standard array processing package\n",
    "import numpy as np\n",
    "# Plotting library\n",
    "from matplotlib import pyplot as plt\n",
    "# Setting the default colormap for pyplot\n",
    "import matplotlib as mpl\n",
    "mpl.rc('image', cmap='gray')\n",
    "# File path processing package\n",
    "from pathlib import Path\n",
    "\n",
    "# Package for some simple biometric metrics. \n",
    "# Of course you can use the code you have developed in the previous assignment\n",
    "from sklearn.metrics import roc_curve\n",
    "# Pickle allows to save and read intermediate results (similar to save and load in Matlab)\n",
    "import pickle\n",
    "# A visual progress bar library\n",
    "from tqdm.notebook import tqdm as tqdm_notebook\n",
    "# Allows us to generate markdown using python code\n",
    "from IPython.display import Markdown\n",
    "# Data analysis and manipulation tool\n",
    "import pandas as pd\n",
    "# Cartesian product of 2 iterables\n",
    "from itertools import product\n",
    "# Path name pattern searching\n",
    "import glob\n",
    "# OS interfaces\n",
    "import os\n",
    "# Compression interface\n",
    "import zlib\n",
    "# Binary encoding\n",
    "from base64 import urlsafe_b64encode as b64e, urlsafe_b64decode as b64d\n",
    "# Some utility functions, you can take a look into these if you're interested in how the code works\n",
    "from src.utils import show,draw_orientations,gabor_kernel,draw_minutiae,angle_mean,angle_abs_difference\n",
    "# The fingerprint enhancement pipeline\n",
    "from src.fingerprint_enhancer import FingerprintImageEnhancer\n",
    "\n",
    "import math\n",
    "\n",
    "# Your imports here (if any)\n",
    "# import ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def plot_image_sequence(data, n, imgs_per_row=7, figsize=(10,10), cmap='gray'):\n",
    "    n_rows = 1 + int(n/(imgs_per_row+1))\n",
    "    n_cols = min(imgs_per_row, n)\n",
    "\n",
    "    f,ax = plt.subplots(n_rows,n_cols, figsize=(figsize[0]*n_cols,figsize[1]*n_rows))\n",
    "    for i in range(n):\n",
    "        if n == 1:\n",
    "            ax.imshow(data[i], cmap=cmap)\n",
    "        elif n_rows > 1:\n",
    "            ax[int(i/imgs_per_row),int(i%imgs_per_row)].imshow(data[i], cmap=cmap)\n",
    "        else:\n",
    "            ax[int(i%n)].imshow(data[i], cmap=cmap)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='I.-Setting'></a>I. Setting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh no! A woman is found dead in her hotel room. Multiple stab wounds in the chest indicate that it was... *MURDER!*\n",
    "\n",
    "The police has locked down the hotel and you were brought in to assist the forensics team in finding the murderer. Your associates have reviewed the security footage and have detected a person entering the room with the victim just 15 minutes before the estimated time of death. Unfortunately, the perpetrator did their homework and they were very careful not to give away any leads that might help you identify them. They slipped however, and on their way out touched the door knob with their bare hand..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='II.-Fingerprint-Recognition'></a> II. Fingerprint Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Reading data\n",
    "#### *Reading the image data and converting to grayscale*\n",
    "\n",
    "\n",
    "Before you get started on anything it is always a good idea to visualise your data. Let's first have a look at the fingerprint that was collected from the door knob first... \n",
    "\n",
    "Note that we convert the data to grayscale, we're not interested in color values (and don't have them)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perpetrator_fp = cv2.imread('./perpetrator_fp.png',cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "\n",
    "show((perpetrator_fp, \"Fingerprint of the horrible person\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's have a look at the other fingerprint data that was provided to you. This data is stores in `data/NIST301/`. There were apparently 100 guests in the hotel at the time of murder. The filename indicates the subject identifier. Below we have provided a function to read and label these fingerprints. Let's take a look!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Here are some samples from the database, which contains 100 images of size 480x320. Remember, visualization is always a good idea, so let's visualize some"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read_DB(path):\n",
    "    images = []\n",
    "    labels = []\n",
    "    imagePaths = sorted(Path(path).rglob(\"*.png\"))\n",
    "    for imagePath in imagePaths:\n",
    "        image = cv2.imread(path + imagePath.name, cv2.IMREAD_GRAYSCALE)\n",
    "        if (len(image.shape) > 2):\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        images.append(image)\n",
    "        label = imagePath.stem[0:3]\n",
    "        labels.append(label)\n",
    "    return (images, labels)\n",
    "\n",
    "\n",
    "# Read the fingerprint database\n",
    "images_db, labels_db = read_DB('./data/NIST301/')\n",
    "\n",
    "# Save some metadata\n",
    "n_imgs = len(images_db)\n",
    "img_height, img_width = images_db[0].shape\n",
    "\n",
    "Markdown(f\"\"\"Here are some samples from the database, which contains {n_imgs} images of size {img_height}x{img_width}. Remember, visualization is always a good idea, so let's visualize some\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 14\n",
    "plot_image_sequence(images_db,n) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. A baseline model\n",
    "\n",
    "#### 2.1. Choosing a similarity metric\n",
    "In your first assignment the similarity metric was already provided, in this assignment you have the chance to play around with distances and similarities yourself! Let's give it a shot!\n",
    "\n",
    "Consider the similarity function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mss = lambda x,y: 1/(1+np.square(x-y).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2. Constructing the similarity score table\n",
    "Since we're not in an authentication scenario, all we can do is to calculate the similarity scores between each fingerprint in the database and the one from the perpetrator.\n",
    "Once we have a metric for each image, we can just sort them and store them in a table. Note that you have space to get creative with visualization!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def constructSimilarityTable(org_img,img_db, labels, dist_func):\n",
    "    #dist_func is the function that computes the distance between two images\n",
    "    data=[]\n",
    "    for i,img in enumerate(tqdm_notebook(img_db)):\n",
    "        data.append([\n",
    "            labels[i],\n",
    "            dist_func(org_img, img)])\n",
    "    assert (len(data) == len(img_db))\n",
    "    return pd.DataFrame(data, columns=['id', 'score'])\n",
    "\n",
    "sim_tb0 = constructSimilarityTable(perpetrator_fp,images_db, labels_db, mss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids,scores = sim_tb0.sort_values(by='score', ascending=False).values[:,0],sim_tb0.sort_values(by='score', ascending=False).values[:,1]\n",
    "plt.plot(ids,scores)\n",
    "plt.xticks(np.arange(0,100,5),ids[np.arange(0,100,5)],rotation = 45)\n",
    "plt.title('Highest Score ID: '+ids[0]);\n",
    "\n",
    "Markdown(f'''We can see that the highest match score {scores[0]:.4f} \n",
    "belongs to subject '''+ids[0]+'. However, subject '+ids[1]+f''' is not far behind with \n",
    "a score of {scores[1]:.4f}! Surely we can do a better job than that. Or can we..?''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Q1: </b> Now that we have some results, is the given similarity function a good metric to quantify a distance between two fingerprints? Is it reliable enough to incriminate a suspect? What are its limitations?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Minutiae-Based Matching\n",
    "\n",
    "We'll be using the tried-and-true method of minutiae detection in order to extract usable features from the images. This will help us have data that is consistent across images.\n",
    "\n",
    "#### 3.1. Image Enhancement\n",
    "In order to make life easy on ourselves in this investigation, we will first start by enhancing the fingerprint images. \n",
    "First thing we should focus on is getting crisp ridge images. In order to do that we have to perform a couple of steps:\n",
    "\n",
    "1. Segment the part of the image that actually contains the fingerprint\n",
    "2. Estimate the ridge orientations\n",
    "3. Estimate the local ridge frequencies\n",
    "\n",
    "To perform these steps, we will be using a mix of code provided by [Utkarsh Deshmukh](https://github.com/Utkarsh-Deshmukh/Fingerprint-Enhancement-Python) and [BioLab Univerity of Bologna](https://colab.research.google.com/drive/1u5X8Vg9nXWPEDFFtUwbkdbQxBh4hba_M#scrollTo=LSe9qJ63zw_E).\n",
    "Let's start with the segmentation. This step is fairly straight-forward. The foreground (fingerprint) contains many edges and the background does not. So, we will use a very simple technique based on the magnitude of the local gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the local gradient (using Sobel filters)\n",
    "fe = FingerprintImageEnhancer()\n",
    "\n",
    "def get_mask(image):\n",
    "    normalized_img,mask = fe.ridge_segment(image)\n",
    "    return normalized_img,mask\n",
    "\n",
    "normalized_img,mask = get_mask(perpetrator_fp)\n",
    "masked_fp = perpetrator_fp * mask\n",
    "show((perpetrator_fp, 'Original'), (masked_fp, 'Masked'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now moving onto estimating the local ridge orientations. This step is essentially finding the angle the fingerprint ridges form with the horizontal axis in a small neighborhood. \n",
    "\n",
    "For each pixel, we will estimate the local orientation from the gradient $[Gx,Gy]$, which we already computed in the segmentation step (see *A.M. Bazen and S.H. Gerez, \"Systematic methods for the computation of the directional fields and singular points of fingerprints,\" in IEEE tPAMI, July 2002*). \n",
    "\n",
    "The ridge orientation is estimated as ortoghonal to the gradient orientation, averaged over a window $W$.  \n",
    "\n",
    "$G_{xx}=\\sum_W{G_x^2}$, $G_{yy}=\\sum_W{G_y^2}$, $G_{xy}=\\sum_W{G_xG_y}$\n",
    "\n",
    "$\\theta=\\frac{\\pi}{2} + \\frac{phase(G_{xx}-G_{yy}, 2G_{xy})}{2}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_orientations():\n",
    "    orientations = fe.ridge_orient()\n",
    "    return orientations\n",
    "\n",
    "orientations = get_orientations()\n",
    "show(draw_orientations(perpetrator_fp, orientations/(-np.pi/3), np.ones_like(orientations)*0.5, mask, 1, 18), 'Orientation image')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the orientatitons are generally pointing in the right directions, even though they dont overlap with the ridges exactly. This is because the orientations are actually a vector field, and we just sample from this field."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all we need is to calculate the ridge frequency. We define the local ridge frequency as the number of ridges per unit length along a hypothetical small window. Initially, we will just assume constant frequency over the entire image and estimate the ridge period on within a set region. Then we reciprocate to get the frequency.\n",
    "\n",
    "We do this by using the *x-signature* of the region. \n",
    "\n",
    "*L. Hong, Y. Wan and A. Jain, \"Fingerprint image enhancement: algorithm and performance evaluation,\" in IEEE tPAMI, Aug. 1998*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = perpetrator_fp[310:390,180:230]\n",
    "smoothed = cv2.blur(region, (5,5), -1)\n",
    "xs = np.sum(smoothed, 1) # the x-signature of the reg\n",
    "x = np.arange(region.shape[0])\n",
    "local_maxima = np.nonzero(np.r_[False, xs[1:] > xs[:-1]] & np.r_[xs[:-1] >= xs[1:], False])[0]\n",
    "f, axarr = plt.subplots(1,2, sharey = True)\n",
    "axarr[0].imshow(region,cmap='gray')\n",
    "axarr[1].plot(xs, x)\n",
    "axarr[1].set_ylim(region.shape[0]-1,0)\n",
    "axarr[1].set_xticks(local_maxima)\n",
    "axarr[1].grid(True, axis='y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This sample shows how the ridge frequency is estimated. We could stop here, but notice how this process will yield different frequencies for each region we select. That is something we do not want. Instead, we should convolve over the image and estimate all local ridge frequencies to create a field that we can later sample for each pixel. We can once again use the provided code for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "def get_ridge_period(image):\n",
    "\n",
    "    freq = fe.ridge_freq()\n",
    "    return freq\n",
    "\n",
    "ridge_freq = get_ridge_period(perpetrator_fp) # Using the transpose for presentation purposes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(255*(ridge_freq/ridge_freq.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The mean ridge frequency is 0.13. This means there is a ridge every 7.62 pixels over all the image.\n",
       "         Note how the output mostly corresponds to the mask. This shows how the frequencies are gathered on the fingerprint ridges, unsurprisingly."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(f'''The mean ridge frequency is {fe._mean_freq:.2f}. This means there is a ridge every {1/(fe._mean_freq):.2f} pixels over all the image.\n",
    "         Note how the output mostly corresponds to the mask. This shows how the frequencies are gathered on the fingerprint ridges, unsurprisingly.''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now that we have all we need to enhance our images, we can start off by creating a Gabor filter bank that will accentuate the details of the fingerprint.\n",
    "\n",
    "This step is actually a contextual convolution, where a different filter will be used for each pixel, based on the orientation value at the given point. By rotating the Gabor filters in accordance with the orientations we estimated, we make sure they capture the ridge. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gabor_filtering(image):\n",
    "    ridge_im, gabor_filter = fe.ridge_filter()\n",
    "    return gabor_filter,ridge_im\n",
    "\n",
    "gabor_bank, enhanced = gabor_filtering(perpetrator_fp)\n",
    "\n",
    "\n",
    "show(perpetrator_fp, enhanced)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(*gabor_bank[0::5])\n",
    "show(*[cv2.filter2D(perpetrator_fp, cv2.CV_32F, gb) for gb in gabor_bank[0::5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now apply the enhancement pipeline to all images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcuate the enhanced images and the associated segmentation masks\n",
    "\n",
    "def enhance_images(images):\n",
    "    images_e_u = []\n",
    "    masks = []\n",
    "    orientations = []\n",
    "    for i, image in enumerate(tqdm_notebook(images)):\n",
    "        try:\n",
    "            images_e_u.append(fe.enhance(image))\n",
    "            masks.append(fe._mask)\n",
    "        except:\n",
    "            print('error for: ', i)\n",
    "    return np.array(images_e_u), np.array(masks)\n",
    "\n",
    "try:\n",
    "    with open('enhanced_images.pkl', 'rb') as f:\n",
    "        images_enhanced_db,masks_db= pickle.load(f)\n",
    "except:\n",
    "    images_enhanced_db, masks_db = enhance_images(images_db)\n",
    "    with open('enhanced_images.pkl', 'wb') as f:\n",
    "        pickle.dump((images_enhanced_db, masks_db), f)\n",
    "        \n",
    "# images_enhanced_db, masks_db = enhance_images(images_db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Tip:</b> Intermediate (computation heavy) results can be saved on file using the pickle package, use <code>pickle.dump</code> to save and <code>pickle.load</code> to load data from a file. The code snippet below will run the computation and save results if there are no previous records, and reload them in later runs. \n",
    "Have a look at the <a href=\"https://wiki.python.org/moin/UsingPickle\">wiki</a> for more information. \n",
    "<pre>\n",
    "<code>\n",
    "<!-- language: lang-python -->\n",
    "try:\n",
    "    with open(\"images_masks_db.pkl\",\"rb\") as f:\n",
    "        images_enhanced_db, masks_db = pickle.load(f)\n",
    "except:\n",
    "    images_enhanced_db, masks_db = enhance_images(images_db)\n",
    "    with open(\"images_masks_db.pkl\",'wb') as f:\n",
    "        pickle.dump([images_enhanced_db,masks_db],f)\n",
    "</code>\n",
    "</pre>\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=4\n",
    "images_segmented_db = [a*b for a,b in zip(images_enhanced_db,masks_db)]\n",
    "plot_image_sequence(images_db[:n] + images_enhanced_db.tolist()[:n] + masks_db.tolist()[:n], 3*n, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Q2: </b> Take a look at the enhanced images. Do you see any challenges that you can face working with these?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2. Minutiae Detection\n",
    "\n",
    "Now that we have a clear image without noise, we can proceed to actually detect minutiae positions and directions. Starting off by generating a skeleton (not the spooky kind) where each line is one pixel wide, we will use a simple method of detecting minutiae. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skeletonize(image):\n",
    "    try:\n",
    "        return cv2.ximgproc.thinning(image,thinningType=cv2.ximgproc.THINNING_GUOHALL)\n",
    "    except:\n",
    "        image = 255*(image).astype(np.uint8)\n",
    "        return cv2.ximgproc.thinning(image,thinningType=cv2.ximgproc.THINNING_GUOHALL)\n",
    "skeleton = skeletonize(enhanced)\n",
    "show(skeleton,\"Skeleton\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To write our algorithm, we need to determine the criteria for terminations and bifurcations. We can do this in the following way:\n",
    "\n",
    "-   For each pixel in the skeleton, we examine its immediate neighborhood of 8-pixels.\n",
    "-   By definition, a termination pixel must have only one neighboring white pixel. So we look for white pixels with one white neighbor for terminations.\n",
    "-   Bifurcations on the other hand, must have one pixel on the tail, and two on the fork. So we look for white pixels with three white neighbors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Q3: </b> Write your detection algorithm below. Keep in mind when you're inspecting the 8-neighborhood centered on a white pixel, the area also includes the pixel you're inspecting. Don't forget to update the criteria with this in mind!\n",
    "\n",
    "Explain in your report how your algorithm works and show some example(s).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_minutiae(skeleton):\n",
    "    ''' \n",
    "    Write your algortihm here to generate minutiae from the skeleton image.\n",
    "    The return of this function should be a Python list of tuples in the form of:\n",
    "    (x-coordinate, y-coordinate, True if termination False if bifurcation)    \n",
    "    '''\n",
    "\n",
    "    return minutiae\n",
    "\n",
    "minutiae = get_all_minutiae(skeleton)\n",
    "\n",
    "show(draw_minutiae(skeleton,minutiae), \"Minutiae\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There will be a lot of false detections with the above step on the borders of the fingerprint. We can get rid of these by applying an eroded mask, such that the minutiae too close to the mask border are excluded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def border_reduce(image,minutiae):\n",
    "    gx, gy = cv2.Sobel(image, cv2.CV_32F, 1, 0), cv2.Sobel(image, cv2.CV_32F, 0, 1)\n",
    "    gx2, gy2 = gx**2, gy**2\n",
    "    gm = np.sqrt(gx2 + gy2)\n",
    "    sum_gm = cv2.boxFilter(gm, -1, (25, 25), normalize = False)\n",
    "    thr = sum_gm.max() * 0.2\n",
    "    mask = cv2.threshold(sum_gm, thr, 255, cv2.THRESH_BINARY)[1].astype(np.uint8)\n",
    "    mask_distance = cv2.distanceTransform(cv2.copyMakeBorder(255*(mask).astype(np.uint8), 1, 1, 1, 1, cv2.BORDER_CONSTANT), cv2.DIST_L1, 3)[1:-1,1:-1]\n",
    "    filtered_minutiae = list(filter(lambda m: mask_distance[m[1], m[0]]>12, minutiae))\n",
    "    return mask_distance,filtered_minutiae\n",
    "\n",
    "mask_distance,filtered_minutiae = border_reduce(perpetrator_fp,minutiae)\n",
    "\n",
    "show((mask,\"Mask\"), (mask_distance,\"Transform\"),(draw_minutiae(skeleton, filtered_minutiae),\"Filtering Result\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To estimate the directions of the minutiae, we can do a couple of things. We can simply sample the orientation map we generated at minutiae locations.   However, this would mean we assume that all minutiae we detected, and that is most likely not the case. Even with all the preprocessing and enhancements we made, there are likely to be some artifactual points, such as very short lines, or just noisy areas. \n",
    "\n",
    "To remedy this, we need an additional filtering step. So we propose following rules.\n",
    "\n",
    "- For terminations, the ridge must continue for 20 pixels from the minutia point, uninterrupted.  \n",
    "- For bifurcations, each arm stemming from the minutia must adhere to the termination rule.\n",
    "- If another minutia is detected within 10 steps of following the ridge, the original minutia is invalid.\n",
    "\n",
    "This filtering method can also be overloaded in order to acquire the directions of the minutiae! By calculating the angle between our starting point, and the point we land on, we can find the direction of a termination. For bifurcations, we will just get the average angle of the two closest angles that we find. \n",
    "\n",
    "<p>\n",
    "    <img src=\"https://biolab.csr.unibo.it/samples/fr/images/min_directions.png\">\n",
    "</p>\n",
    "<p>\n",
    "    <em> Image from BioLab, University of Bologna </em>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "This is a fairly complex process to code, so we provide you with the functions you will need. Feel free to dig in and explore!\n",
    "\n",
    "Usage of the provided code:\n",
    "<pre>\n",
    "<code>\n",
    "mf = MinutiaeFilter() # You need to do this just once, not for all images\n",
    "valid_minutiae_filtered = mf.get_filtered(enhanced_image) # Using the enhanced image, not the original\n",
    "</code>\n",
    "</pre>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MinutiaeFilter:\n",
    "    cn_filter = np.array([[  1,  2,  4],\n",
    "                    [128,  0,  8],\n",
    "                    [ 64, 32, 16]\n",
    "                    ])\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def __compute_crossing_number(self,values):\n",
    "        return np.count_nonzero(values < np.roll(values, -1))\n",
    "    \n",
    "    def __set_stage(self,skel):\n",
    "        self.all_8_neighborhoods = [np.array([int(d) for d in f'{x:08b}'])[::-1] for x in range(256)]\n",
    "        self.cn_lut = np.array([self.__compute_crossing_number(x) for x in self.all_8_neighborhoods]).astype(np.uint8)\n",
    "        \n",
    "        skel01 = np.where(skel!=0, 1, 0).astype(np.uint8)\n",
    "        # Apply the filter to encode the 8-neighborhood of each pixel into a byte [0,255]\n",
    "        self.neighborhood_values = cv2.filter2D(skel01, -1, MinutiaeFilter.cn_filter, borderType = cv2.BORDER_CONSTANT)\n",
    "        self.cn = cv2.LUT(self.neighborhood_values, self.cn_lut)\n",
    "        # Keep only crossing numbers on the skel\n",
    "        self.cn[skel==0] = 0\n",
    "\n",
    "        r2 = 2**0.5 # sqrt(2)\n",
    "        # The eight possible (x, y) offsets with each corresponding Euclidean distance\n",
    "        self.xy_steps = [(-1,-1,r2),( 0,-1,1),( 1,-1,r2),( 1, 0,1),( 1, 1,r2),( 0, 1,1),(-1, 1,r2),(-1, 0,1)]\n",
    "        # LUT: for each 8-neighborhood and each previous direction [0,8],\n",
    "        #      where 8 means \"none\", provides the list of possible directions\n",
    "        self.nd_lut = [[self.compute_next_ridge_following_directions(pd, x) for pd in range(9)] for x in self.all_8_neighborhoods]\n",
    "            \n",
    "    \n",
    "    def get_filtered(self,image_enhanced):\n",
    "        \n",
    "        skel = skeletonize(image_enhanced)\n",
    "\n",
    "        self.__set_stage(skel)\n",
    "        minutiae = get_all_minutiae(skel)\n",
    "        try: \n",
    "            _,filtered_minutiae = border_reduce(image_enhanced,minutiae)\n",
    "        except:\n",
    "            _,filtered_minutiae = border_reduce(255*image_enhanced.astype(np.uint8),minutiae)\n",
    "        valid_minutiae = []\n",
    "        for x, y, term in filtered_minutiae:\n",
    "            d = None\n",
    "            if term: # termination: simply follow and compute the direction\n",
    "                d = self.follow_ridge_and_compute_angle(x, y)\n",
    "            else: # bifurcation: follow each of the three branches\n",
    "                dirs = self.nd_lut[self.neighborhood_values[y,x]][8] # 8 means: no previous direction\n",
    "                if len(dirs)==3: # only if there are exactly three branches\n",
    "                    angles = [self.follow_ridge_and_compute_angle(x+self.xy_steps[d][0], y+self.xy_steps[d][1], d) for d in dirs]\n",
    "                    if all(a is not None for a in angles):\n",
    "                        a1, a2 = min(((angles[i], angles[(i+1)%3]) for i in range(3)), key=lambda t: angle_abs_difference(t[0], t[1]))\n",
    "                        d = angle_mean(a1, a2)\n",
    "            if d is not None:\n",
    "                valid_minutiae.append( (x, y, term, d) )\n",
    "\n",
    "        return valid_minutiae\n",
    "    \n",
    "    def compute_next_ridge_following_directions(self,previous_direction, values):\n",
    "        next_positions = np.argwhere(values!=0).ravel().tolist()\n",
    "        if len(next_positions) > 0 and previous_direction != 8:\n",
    "            # There is a previous direction: return all the next directions, sorted according to the distance from it,\n",
    "            #                                except the direction, if any, that corresponds to the previous position\n",
    "            next_positions.sort(key = lambda d: 4 - abs(abs(d - previous_direction) - 4))\n",
    "            if next_positions[-1] == (previous_direction + 4) % 8: # the direction of the previous position is the opposite one\n",
    "                next_positions = next_positions[:-1] # removes it\n",
    "        return next_positions\n",
    "\n",
    "    def follow_ridge_and_compute_angle(self,x, y, d = 8):\n",
    "        px, py = x, y\n",
    "        length = 0.0\n",
    "        while length < 20: # max length followed\n",
    "            next_directions = self.nd_lut[self.neighborhood_values[py,px]][d]\n",
    "            if len(next_directions) == 0:\n",
    "                break\n",
    "            # Need to check ALL possible next directions\n",
    "            if (any(self.cn[py + self.xy_steps[nd][1], px + self.xy_steps[nd][0]] != 2 for nd in next_directions)):\n",
    "                break # another minutia found: we stop here\n",
    "            # Only the first direction has to be followed\n",
    "            d = next_directions[0]\n",
    "            ox, oy, l = self.xy_steps[d]\n",
    "            px += ox ; py += oy ; length += l\n",
    "        # check if the minimum length for a valid direction has been reached\n",
    "        return math.atan2(-py+y, px-x) if length >= 10 else None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Q4: </b> Comment on the advantages of detecting the orientations of minutiae on top of just locations.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mf = MinutiaeFilter()\n",
    "valid_minutiae = mf.get_filtered(enhanced)\n",
    "\n",
    "show(draw_minutiae(skeleton,filtered_minutiae),draw_minutiae(skeleton,valid_minutiae),draw_minutiae(perpetrator_fp, valid_minutiae))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whew, those were some long steps to get robust minutiae! But now that we have a method of generating them, we need to do come up with a way to match these, to find our perpetrator and bring him to justice!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3. Global Matching and Image Alignment\n",
    "\n",
    "Here we use the matching keypoints to align the fingerprints, this allows us to compare the images as a whole (globally). We will first align our images with respect to the local descriptors around our minutiae, then we will compare the geometric distances between the matching sets to compute similarity.\n",
    "\n",
    "[Here](https://www.learnopencv.com/image-alignment-feature-based-using-opencv-c-python/) you can find a description and code how to start from the brute force matching results and estimate the best transformation (from a family of transformations) that aligns the two images. In the example, a homography-type transformation is searched for. However, this has too many degrees of freedom for our application. We substituted this by a more constrained (only 4 degrees of freedom) similarity (partial affine) transformation.\n",
    "\n",
    "These routines iteratively determine the minimal set of matching points that define a transformation that optimally aligns all other points as well, taking care of outliers at the same time. This method is a very general optimization technique and is called RANSAC, for \"RANdom SAmple Consensus\". See, apart from many other sources on the internet, [this presentation](http://www.cse.psu.edu/~rtc12/CSE486/lecture15.pdf) for further explanation. \n",
    "\n",
    "In order to use our minutiae with the OpenCV matching pipeline, we have to pull a fast one on OpenCV. We do a cheeky KeyPoint conversion with [<code>cv2.KeyPoint</code>](https://docs.opencv.org/3.4/d2/d29/classcv_1_1KeyPoint.html), set the coordinates to those of our minutiae, and use our minutiae as just as we would use <code>cv2.KeyPoint</code>.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minutia2kp(minutiae):\n",
    "    return [cv2.KeyPoint(x, y, 10, angle, 0, 0, -1) for x, y, _, angle in minutiae] #no angle or type for the keypoints..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![hehe](./img/meme.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can feed the minutiae to the detectors to get our local descriptors, and ultimately use these descriptors for image alignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testNr1 = 6\n",
    "testNr2 = 18\n",
    "\n",
    "def compute_local_descriptor(img, minutiae):\n",
    "    detector = cv2.ORB_create()\n",
    "    kp = minutia2kp(minutiae)\n",
    "    kp, des = detector.compute(img, kp)\n",
    "    return np.array(kp), des\n",
    "\n",
    "mn1 = mf.get_filtered(images_enhanced_db[testNr1])\n",
    "mn2 = mf.get_filtered(images_enhanced_db[testNr2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "mn1 , local_des1 = compute_local_descriptor(images_enhanced_db[testNr1],mn1 )\n",
    "mn2 ,local_des2 = compute_local_descriptor(images_enhanced_db[testNr2],mn2 )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The Local Feature descriptor** will give you a vector describing the local region around the minutia in the feature space. Note that these feature descriptors give a vectorial summary of the neighbourhood around minutiae. A simple metric on these vectors (Euclidean Distance for continuous variables, Hamming Distance for binary variables) can then be used to determine similarity. These are what the matcher will compare when evaluating a pair of minutiae."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def brute_force_matcher(des1, des2, dist=cv2.NORM_HAMMING):\n",
    "    \"\"\"\n",
    "      Brute Force matcher on a pair of KeyPoint (hehe) sets using the local descriptor for similarity\n",
    "      \n",
    "      returns all pairs of best matches\n",
    "    \"\"\"\n",
    "    \n",
    "    # crossCheck=True only retains pairs of keypoints that are each other best matching pair\n",
    "    bf = cv2.BFMatcher(dist, crossCheck=True)\n",
    "    matches = list(bf.match(des1, des2))\n",
    "  \n",
    "    # sort matches based on descriptor distance\n",
    "    matches.sort(key=lambda x: x.distance, reverse=False)\n",
    "    \n",
    "    return np.array(matches)\n",
    "\n",
    "local_k1_k2_matches = brute_force_matcher(local_des1, local_des2,cv2.NORM_HAMMING)\n",
    "\n",
    "local_pt_source, local_pt_target = np.array([\n",
    "    (match.queryIdx, match.trainIdx) for match in local_k1_k2_matches]).T\n",
    "\n",
    "def estimate_affine_transform_by_kps(src_pts, dst_pts):\n",
    "    \"\"\"\n",
    "        Returns the Affine transformation that aligns two sets of points\n",
    "    \"\"\"\n",
    "    transform_matrix, inliers = cv2.estimateAffinePartial2D(src_pts, dst_pts, \n",
    "                                           method =  cv2.RANSAC, \n",
    "                                           confidence = 0.9, \n",
    "                                           ransacReprojThreshold = 10.0, \n",
    "                                           maxIters = 5000, \n",
    "                                           refineIters = 10)\n",
    "    return transform_matrix, inliers[:,0]\n",
    "\n",
    "def warp_points(pts, M):\n",
    "    mat_reg_points = cv2.transform(pts.reshape(-1,1,2), M)\n",
    "\n",
    "    # return transformed keypoint list\n",
    "    return cv2.KeyPoint.convert(np.array(mat_reg_points).reshape(-1,2))\n",
    "\n",
    "\n",
    "def warp_img(img, M):\n",
    "    return cv2.warpAffine(img, M, (img.shape[1], img.shape[0]))\n",
    "\n",
    "# estimate the affine transform that aligns the matched keypoints\n",
    "M, inliers = estimate_affine_transform_by_kps(\n",
    "    cv2.KeyPoint.convert(np.array(mn1)[local_pt_source]), \n",
    "    cv2.KeyPoint.convert(np.array(mn2)[local_pt_target]))\n",
    "\n",
    "mn1_reg = warp_points(cv2.KeyPoint.convert(np.array(mn1)), M)\n",
    "img1_reg =  warp_img(images_enhanced_db[testNr1], M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Point Matchers** match points that have similar descriptors in an image pair and computes the distances between the best matching pairs of keypoints. In this implementation we make use of the brute force matcher of OpenCV, documentation can be found [here](https://opencv24-python-tutorials.readthedocs.io/en/latest/py_tutorials/py_feature2d/py_matcher/py_matcher.html). As a distance between descriptors we make use of the normalised [Hamming distance](https://en.wikipedia.org/wiki/Hamming_distance). Usually you will see these matchers used with keypoint detection algorithms such as SIFT, but in our case we have a more reliable way of locating points of interest!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at how the images are alinged. Try visualizing different pairs of fingerprints! You may notice the estimated transform will be more extreme for images that are less similar, and will become more subtle as the images become more similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_img1 = cv2.drawKeypoints(img1_reg, mn1_reg, None, (0, 255, 0), cv2.DRAW_MATCHES_FLAGS_DEFAULT)\n",
    "show_img2 = cv2.drawKeypoints(images_enhanced_db[testNr2], mn2, None, (0, 255, 0), cv2.DRAW_MATCHES_FLAGS_DEFAULT)\n",
    "\n",
    "show(show_img1, show_img2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you might have noticed the `estimateAffinePartial2D` function does not only return the transformation matrix but also the minimal set of matching points that were used to to determine the transformation. This gives us a better, more refined set of keypoints. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_k1_k2_matches = local_k1_k2_matches[inliers == 1]\n",
    "\n",
    "imMatches = cv2.drawMatches(img1_reg, mn1_reg, images_enhanced_db[testNr2], mn2, global_k1_k2_matches.tolist(), None)\n",
    "show(imMatches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Q5:</b> Are all the keypoint matches accurate? Are they expected to be? Explain why.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4. Global Similarity Metric\n",
    "Now that we have matching keypoints we can define simple scalar measures on this set of distances, such as the number of pairs with a distance smaller than a set threshold, or the sum/mean of the first N distances (ranked from small to larger), etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Q6: </b> Choose a global feature similarity function, (e.g. you can start from euclidean distance between the reduced sets of KeyPoints and count the values above a threshold).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Tip:</b> Make sure you spent the proper amount of time on constructing the similarity metrics, a good implementation will save you a lot of time in the long run! </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def global_img_similarity(matches, reg_kp1, kp2):\n",
    "    '''\n",
    "    Given the matches, and more importantly the matching minutiae,\n",
    "    fill out this function to return a similarity score between the two.\n",
    "    '''\n",
    "    \n",
    "    return sim_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perp_img_en,perp_img_mask = 255*enhanced.astype(np.uint8), mask\n",
    "\n",
    "def global_similarity(masked_fp1, masked_fp2, detector=cv2.ORB_create(), kp_erosion_ksize = (5,5)):\n",
    "    # separate the semgentation from the image\n",
    "    fp1, mask1 = masked_fp1[...,0], masked_fp1[...,1] \n",
    "    fp2, mask2 = masked_fp2[...,0], masked_fp2[...,1] \n",
    "\n",
    "    \n",
    "    # detect the keypoints\n",
    "    mn1 = mf.get_filtered(fp1)\n",
    "    mn2 = mf.get_filtered(fp2)\n",
    "\n",
    "    # compute descriptor for each keypoints\n",
    "    \n",
    "    mn1, local_des1 = compute_local_descriptor(fp1, mn1)\n",
    "    mn2, local_des2 = compute_local_descriptor(fp2, mn2)\n",
    "    \n",
    "    # find matches between keypoints based on local feature descriptor\n",
    "    local_k1_k2_matches = brute_force_matcher(local_des1, local_des2, cv2.NORM_HAMMING)\n",
    "    \n",
    "    # get source and target index for each match\n",
    "    local_pt_source, local_pt_target = np.array([\n",
    "        (match.queryIdx, match.trainIdx) for match in local_k1_k2_matches]).T\n",
    "    \n",
    "    # use matching keypoints to estimate an affine transform between keypoints\n",
    "    M, inliers = estimate_affine_transform_by_kps(\n",
    "        cv2.KeyPoint.convert(mn1[local_pt_source]), \n",
    "        cv2.KeyPoint.convert(mn2[local_pt_target]))\n",
    "    \n",
    "    # if no inliers can be found\n",
    "    if M is None: \n",
    "        return 0\n",
    "    \n",
    "    # warp the keypoints according to the found transform\n",
    "    mn1_reg = warp_points(cv2.KeyPoint.convert(mn1), M)\n",
    "    \n",
    "    # subset the keypoints, inliers are considered good keypoints\n",
    "    # since they were used in finding the transformation\n",
    "    global_k1_k2_matches = local_k1_k2_matches[inliers == 1]\n",
    "    \n",
    "    # compute global similarity based aligned matching global keypoints\n",
    "    return global_img_similarity(global_k1_k2_matches,mn1_reg, mn2)\n",
    " \n",
    "\n",
    "sim_tb1 = constructSimilarityTable(np.stack((perp_img_en,perp_img_mask),-1), np.stack((images_segmented_db,masks_db),-1), labels_db, global_similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5. Validation\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Q7: </b> Visualize the scores and determine a score threshold to discriminate the matching fingerprints. Explain how you determine the threshold.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A good idea in this plot, where we cannot pick out one clear subject (as the first 3 subjects are all pretty close), is to take a look at the knee point for a score threshold. Also, let's normalize our scores in order to get a well generalized scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simple module to find the knee in a series\n",
    "from kneed import KneeLocator\n",
    "\n",
    "scores_local = (scores - scores.min()) / (scores.max() - scores.min())\n",
    "\n",
    "kneedle = KneeLocator(list(range(len(ids))),scores_local,S=1, curve='convex', direction='decreasing')\n",
    "print(f'Knee at: {kneedle.knee}\\t Treshold: ',kneedle.knee_y)\n",
    "kneedle.plot_knee()\n",
    "Markdown(f\" With the threshold of {kneedle.knee_y:.3f}, we can see a more clear divide between the subjects. However, the best {kneedle.knee} matches are possible suspects... \" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No clear distinction... This is suspicious, let's take a look at the best matching fingerprints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_by_label(ind):\n",
    "    for i in range(len(labels_db)):\n",
    "        if labels_db[i] == ind:\n",
    "            return images_enhanced_db[i]\n",
    "    raise ValueError(\"No image with this label is found.\")\n",
    "\n",
    "# sim_tb1 should be sorted by score for this cell\n",
    "imgs = [get_image_by_label(i) for i in sim_tb1.values[:10,0]]\n",
    "\n",
    "plot_image_sequence(imgs,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They are all the same fingerprint! The murderer must have hacked the database and replicated some of the images, effectively covering their tracks. This must be an inside job! They seem to be one step ahead... Or so they think! Fingerprints are not the only biometric you can evaluate.\n",
    "\n",
    "Since you can't trust your immediate co-workers anymore (afterall whoever did this had access to your database), you call your friends at CSI New York and send them the video surveillance records. By making use of the revolutionary (and non-existent) technology of ***zoom-and-enhance***, they return to you a database of iris images.\n",
    "\n",
    "![Zoom and Enhance](./img/csi_zoom_enhance.gif)\n",
    "\n",
    "***The plot thickens...***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Iris recognition\n",
    "The investigation is back on track! Now let's create a biometric system for the iris data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Reading data\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Q8: </b> Check out <code>iris_perpetrator.png</code>. Where do you see difficulties? What kind of similarity measures do you expect to work best?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_perpetrator = cv2.cvtColor(cv2.imread(\"perpetrator_iris.bmp\"),cv2.COLOR_BGR2GRAY)\n",
    "show((iris_perpetrator,\"Look at this cold, dead eye...\"));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First things first, let's load the iris images and inspect a few examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set path\n",
    "iris_data_path = 'data/CASIA1'\n",
    "def read_iris_DB(path):\n",
    "    images = []\n",
    "    labels = []\n",
    "    \n",
    "    for dirpath, _, filenames in os.walk(path):\n",
    "        for filename in filenames:\n",
    "            if filename.endswith(\".jpg\"):\n",
    "                label = int(dirpath.split('/')[-1])\n",
    "                im = cv2.imread(os.path.join(dirpath,filename),cv2.IMREAD_GRAYSCALE)\n",
    "                labels.append(label)\n",
    "                images.append(im)\n",
    "    return np.array(images), np.array(labels)\n",
    "\n",
    "\n",
    "# read iris Database\n",
    "iris_images,iris_labels = read_iris_DB(iris_data_path)\n",
    "\n",
    "# shuffle the data\n",
    "idc = np.arange(len(iris_images))\n",
    "np.random.shuffle(idc)\n",
    "iris_images, iris_labels = iris_images[idc], iris_labels[idc]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 7\n",
    "plot_image_sequence(iris_images[10:20], n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Biometric iris Recognition System\n",
    "#### 2.1 Image enhancement\n",
    "\n",
    "We can see the raw images may not be the best for further identification tasks, so we we will segment the irises and preprocess the images. To do this, we will make use of the code by. These modules will help us in the next steps.\n",
    "\n",
    "We can segment the iris out of the image with several steps:\n",
    "1. Edge detection: We can use edge detection to localize the borders. in out image.\n",
    "2. [Hough transform](https://www.sciencedirect.com/topics/computer-science/hough-transforms#:~:text=The%20Hough%20transform%20(HT)%20%5B,the%20Radon%20transform%20%5BDeans81%5D.): This transform will help us detect the round shape of the iris. This way we will be able to select only the region of interest.\n",
    "3. [Daugman Normalization](https://ijsta.com/papers/ijstav1n1/IJSTA_V1N1P3_PP11-14.pdf): We'll use this method to \"unroll\" our iris images to get a nice rectangular segment.\n",
    "\n",
    "P.S. For a more detailed demonstration of these steps, you can refer to this repo by [Sobhan Shukueian](https://github.com/sobhanshukueian/Iris-Identification/blob/main/preprocess.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Tip: </b> The below cell is a lenghthy calculation (~20 min)! If you want adjust parameters or fiddle with the code, it is recommended you do it on a small sample of a dataset, and proceed once you have decided the best setup.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from src.irismodules.fnc import segment, normalize\n",
    "\n",
    "\n",
    "# Feel free to play around with these!\n",
    "\n",
    "eyelashes_thres = 80\n",
    "radial_res = 200\n",
    "angular_res = 500\n",
    "\n",
    "def segment_iris(img):\n",
    "    # Segment the iris region from the eye image. Indicate the noise region.\n",
    "    ciriris, cirpupil, imwithnoise = segment.segment(img,eyelashes_thres=eyelashes_thres)\n",
    "\n",
    "    # Normalize iris region by unwraping the circular region into a rectangular block of constant dimensions.\n",
    "    polar_iris, mask = normalize.normalize(\n",
    "        imwithnoise, ciriris[1], ciriris[0], ciriris[2], cirpupil[1], cirpupil[0], cirpupil[2], radial_res, angular_res)\n",
    "    \n",
    "    return polar_iris, (mask == 0)      \n",
    "\n",
    "def get_filter_bank(ksize = 5,sigma = 4, theta_range = np.arange(0,np.pi,np.pi/16), lambd=10,gamma = 0.5,psi=0 ):\n",
    "    # this filterbank comes from https://cvtuts.wordpress.com/\n",
    "    filters = []\n",
    "    for theta in theta_range:\n",
    "        kern = cv2.getGaborKernel((ksize, ksize), sigma, theta, lambd, gamma, psi, ktype=cv2.CV_32F)\n",
    "        kern /= 1.5*kern.sum()\n",
    "        filters.append(kern)\n",
    "    return filters\n",
    "    \n",
    "def enhance_iris(img, eps = 1.e-15, agg_f = np.max):\n",
    "    # get the gabor filters\n",
    "    filters = get_filter_bank()\n",
    "\n",
    "    # apply filters to image\n",
    "    enhanced_image = np.array([cv2.filter2D(img, ddepth = -1, kernel=k) for k in filters])\n",
    "    \n",
    "    # Normalize in the [0,255] range\n",
    "    enhanced_image = cv2.normalize(enhanced_image, None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX, dtype=0)\n",
    "    \n",
    "    # aggregate features\n",
    "    return agg_f(enhanced_image,0)\n",
    "    \n",
    "def enhance_and_segment_irises(images):\n",
    "    enhanced_images, masks = [], []\n",
    "    for i,img in enumerate(tqdm_notebook(images)):\n",
    "        try:\n",
    "            normalised_img, mask = segment_iris(img)\n",
    "            enhanced_img = enhance_iris(normalised_img)\n",
    "            \n",
    "            masks.append(mask)\n",
    "            enhanced_images.append(enhanced_img)\n",
    "        except:\n",
    "            print(f\"Problem with {i}, skipping\")\n",
    "            continue\n",
    "    return np.array(enhanced_images), np.array(masks)\n",
    "\n",
    "try:\n",
    "    with open(\"iris_mask_db.pkl\",\"rb\") as f:\n",
    "        enhanced_iris,mask_iris,iris_images,iris_labels = pickle.load(f)\n",
    "except:\n",
    "    enhanced_iris, mask_iris = enhance_and_segment_irises(iris_images)\n",
    "    with open(\"iris_mask_db.pkl\",\"wb\") as f:\n",
    "        pickle.dump([enhanced_iris,mask_iris,iris_images,iris_labels],f)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=5\n",
    "iris_segmented_db = np.array([a*b for a,b in zip(enhanced_iris, mask_iris)])\n",
    "plot_image_sequence(enhanced_iris.tolist()[:n] + mask_iris.tolist()[:n] + iris_segmented_db.tolist()[:n], 3*n, n, figsize= (10,5), cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Q9: </b> Inspect the enhanced iris images. Do any of your difficulty predictions still hold? Do you see new ones?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2. Triplet Loss Encoder\n",
    "We're running out of options with regard to what other evidence to evaluate, so we do not want to take any more chances. \n",
    "\n",
    "It's time to bring out the big guns. We will use a Triplet Encoder neural network to catch the murderer.\n",
    "\n",
    "A triplet encoder is a specific type of architecture that has the goal of projecting inputs to a latent space, in which samples of the same class are closer together, and the embeddings of different classes are far apart. \n",
    "We train these kinds of models by using triplets of (anchor, positive, negative). The anchor is the original image, the positive sample is a sample of the same class, and the negative sample is of a different class.\n",
    "\n",
    "<img src=\"./img/triplet.webp\" alt=\"Triplet Model\" width=\"800\"/>\n",
    "\n",
    "The triplet loss function can be represented by the following equation:\n",
    "\n",
    "$\\mathcal{L}(a, p, n) = \\max(0, \\alpha + d(a, p) - d(a, n))$ \n",
    "\n",
    "Where:\n",
    "- $ \\mathcal{L}(a, p, n)$ is the triplet loss for the anchor $ a$, positive sample $ p$, and negative sample $ n$.\n",
    "- $ \\alpha$ is a margin hyperparameter that controls the minimum difference between the distances of positive and negative samples from the anchor. It has to be non-zero, and fairly large to prevent the network from cheating.\n",
    "- $ d(a, p)$ is the distance between the anchor $ a$ and the positive sample $ p$ in the embedding space.\n",
    "- $ d(a, n)$ is the distance between the anchor $ a$ and the negative sample $ n$ in the embedding space.\n",
    "\n",
    "The triplet loss penalizes the model when the distance between the anchor and the positive sample is not smaller than the distance between the anchor and the negative sample by at least \\( \\alpha \\). Otherwise, it incurs no loss. This encourages the model to learn embeddings where similar samples are closer together and dissimilar samples are farther apart. Read more about the triplet loss [here](https://medium.com/deep-learning-hk/compute-document-similarity-using-autoencoder-with-triplet-loss-eb7eb132eb38).\n",
    "\n",
    "We will start off by coding a dataset and a dataloader. We'll be using [PyTorch](https://pytorch.org/tutorials/beginner/pytorch_with_examples.html) for all our neural network needs. It has extensive documentation, so if you have a problem, you can likely find the answer there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms as T #for image transformations\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # no mps, it's very slow. \n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Setting up random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "#split train and valudation sets\n",
    "\n",
    "iris_val_images = np.zeros_like(iris_segmented_db[:len(np.unique(iris_labels))])\n",
    "iris_val_labels = np.zeros_like(np.unique(iris_labels))\n",
    "for i,l in enumerate(np.unique(iris_labels)):\n",
    "    \n",
    "    iris_val_images[i] = iris_segmented_db[iris_labels == l][0]\n",
    "    iris_val_labels[i] = iris_labels[iris_labels == l][0]\n",
    "    \n",
    "    #Remove the validation images from the training set\n",
    "    iris_segmented_db = np.delete(iris_segmented_db, np.argwhere(iris_labels == l)[0], axis = 0)\n",
    "    iris_labels = np.delete(iris_labels, np.argwhere(iris_labels == l)[0])\n",
    "\n",
    "    \n",
    "#Create a Custom Dataset class\n",
    "\n",
    "class IrisDataset(Dataset):\n",
    "    def __init__(self, images, labels, transform=None,device = torch.device(\"cpu\")):\n",
    "        \n",
    "        self.images = torch.tensor(images).unsqueeze(1).to(device)/255\n",
    "        self.labels = torch.tensor(labels).to(device)\n",
    "        self.transform = transform\n",
    "        self.index = torch.arange(self.images.shape[0]).to(device)\n",
    "\n",
    "            \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def get_random_positive(self,label):\n",
    "        # To get random positive, we can't find one in the batch\n",
    "        pos_idx = self.index[self.labels == label]\n",
    "        idx = torch.randint(0,len(pos_idx),(1,))\n",
    "        im = self.images[idx]\n",
    "        return im\n",
    "\n",
    "    def get_random_negative(self,label):\n",
    "        # For random negative if there are no hard negatives in the batch\n",
    "        neg_idx = self.index[self.labels != label]\n",
    "        idx = torch.randint(0,len(neg_idx),(1,))\n",
    "        im = self.images[idx]\n",
    "        return im\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "\n",
    "transform = T.Compose([\n",
    "    T.Resize((100,200))\n",
    "])\n",
    "\n",
    "train_set = IrisDataset(iris_segmented_db, iris_labels, transform=transform, device = device)\n",
    "val_set = IrisDataset(iris_val_images, iris_val_labels, transform=transform ,device = device)\n",
    "\n",
    "batch_size = 256\n",
    "iris_loader = DataLoader(train_set, batch_size=batch_size, shuffle=False)\n",
    "val_loader = DataLoader(val_set, batch_size=len(iris_val_images), shuffle=False)\n",
    "\n",
    "batch_images, batch_labels = next(iter(iris_loader))\n",
    "\n",
    "show(*batch_images[:5,0].cpu().numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a wrapper to get our data in the correct format, we can focus on the model and the training! We will use a convolutional encoder. and train it with a triplet loss. \n",
    "Let's start by defining a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IrisEncoder(nn.Module):\n",
    "    def __init__(self,embed_dim):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1,16,kernel_size=4, stride=2, padding=1), #size//2\n",
    "            nn.MaxPool2d(kernel_size=2), #size//2\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16,32,kernel_size=4, stride=2, padding=1),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(32*6*12,embed_dim) \n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.linear(self.encoder(x).view(-1,32*6*12))\n",
    "        return x\n",
    "\n",
    "Markdown(\"You can see we have two Conv layers with ReLU activations and max pooling. After this initial convolution block, we use a single fully connected layers to get the final embedding.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One more thing before we can actually train our encoder; we need a [triplet mining](https://omoindrot.github.io/triplet-loss#triplet-loss-and-triplet-mining) strategy. We can do random triplet selection, which is fine in some cases, or we can be a little more refined and choose *semi-hard* triplets. This kind of triplet selection gives us a set where the negative sample is not closer to the anchor than to positive, but it still lies within the margin, such as:\n",
    "\n",
    "$d(a, p) < d(a,n) < d(a,p) + \\alpha$ \n",
    "\n",
    "You can think od this as specifically selecting negatives with which the network has trouble, so it can learn to differentiate them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_semihard_triplet_embeddings(batch,labels,model,dataset,device,margin):\n",
    "        embeddings = model(batch.to(device))\n",
    "\n",
    "        #Find the furthest positive for each sample\n",
    "        triplets = torch.zeros(embeddings.shape[0],3,embeddings.shape[1]).to(device)\n",
    "\n",
    "        for i in range(embeddings.shape[0]):\n",
    "            label = labels[i].item()\n",
    "            sample = embeddings[i]\n",
    "            rel = embeddings[labels==label]\n",
    "            pdist = 0\n",
    "            max_d_positive = None\n",
    "            triplets[i,0]=sample #the anchor\n",
    "            #Look for a positive that is furthest from the anchor\n",
    "            for positive in rel:\n",
    "                d = torch.dist(sample,positive)\n",
    "                if d > pdist:\n",
    "                    max_d_positive=positive\n",
    "                    pdist = d\n",
    "            if max_d_positive is None:\n",
    "                # find a random positive if no positive meeting the criteria is found\n",
    "                im= dataset.get_random_positive(label)\n",
    "                embed = model(im.to(device))\n",
    "                max_d_positive = embed\n",
    "            if max_d_positive is None:\n",
    "                raise ValueError(\"No positives found.\")\n",
    "            triplets[i,1]=max_d_positive #the positive\n",
    "\n",
    "            #Find semi-hard negative for each sample\n",
    "            rel = embeddings[labels!=label]\n",
    "            rel_label = labels[labels!=label]\n",
    "            n_dist = 10000\n",
    "            min_d_margin = None\n",
    "            min_d_negative = None\n",
    "            for j,negative in enumerate(rel):\n",
    "                d=torch.dist(sample,negative)\n",
    "                if d<n_dist:\n",
    "                    if pdist<d<pdist+margin:\n",
    "                        min_d_negative=negative \n",
    "                        n_dist = d\n",
    "                        min_d_margin=margin\n",
    "            n_dist = 1000\n",
    "            if min_d_negative is None:\n",
    "                #If no semi-hard negative is found,go for hard negative (closer than the positive)\n",
    "                for j,negative in enumerate(rel):\n",
    "                    d=torch.dist(sample,negative)\n",
    "                    if d<n_dist:\n",
    "                        min_d_negative=negative\n",
    "                        n_dist = d\n",
    "                        min_d_margin=margin\n",
    "            if min_d_negative is None:\n",
    "                #if no hard negatives are found get, like, any negative?\n",
    "                im= dataset.get_random_negative(label)\n",
    "                embed = model(im.to(device))\n",
    "                min_d_negative = embed\n",
    "            if min_d_negative is None:\n",
    "                raise ValueError(\"No negatives found.\")\n",
    "            triplets[i,2]=min_d_negative #the negative\n",
    "        \n",
    "        return triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training setup\n",
    "embed_dim = 64\n",
    "margin = 0.7\n",
    "model = IrisEncoder(embed_dim).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "epochs = 60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we train the encoder model with all the data we have! \n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Tip: </b> When it comes to neural net training, slow and steady usually wins the race. Use a low enough learning rate, and don't be discouraged if the loss doesn't change in the first ~30 epochs. We're trying to approximate a fairly complex function, so it may take a while to see results.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model,loader):\n",
    "    with torch.no_grad():\n",
    "        for val_ims,val_labels in loader:\n",
    "            embeddings = model(val_ims.to(device))\n",
    "            sim = torch.cdist(embeddings,embeddings)\n",
    "            sim = sim.mean()\n",
    "        return sim\n",
    "    \n",
    "\n",
    "#training loop\n",
    "\n",
    "best_model_wts = model.state_dict()\n",
    "best_dst = 0\n",
    "t= tqdm_notebook(range(epochs))\n",
    "for epoch in t:\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(iris_loader):\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        triplets = get_semihard_triplet_embeddings(inputs,labels,model,iris_loader.dataset,device,margin=margin)\n",
    "        \n",
    "        anchor, positive, negative = triplets[:,0],triplets[:,1],triplets[:,2]\n",
    "        l2_diff = ((anchor-positive)**2).sum(dim=1) - ((anchor-negative)**2).sum(dim=1)\n",
    "        loss_triplet = torch.maximum((l2_diff+margin).mean(), torch.tensor(0))\n",
    "        loss_triplet.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss_triplet.item()\n",
    "    val_distance = validate(model,val_loader)\n",
    "    msg= f\"Epoch {epoch+1}, Train Loss: {running_loss/len(iris_loader):.5f}, Mean Val Distance: {val_distance:.3f}\"\n",
    "    if val_distance > best_dst:\n",
    "        best_dst = val_distance\n",
    "        best_model_wts = model.state_dict()\n",
    "        msg=\"*\" + msg\n",
    "\n",
    "    t.set_description(msg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3. Validation\n",
    "Great! Now let's take a look at how well our network learned. \n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Tip: </b> During training, you can use a validation set to track how well your model is doing on unseen data. This allows you to checkpoint your model based on the validation performance. You can use this snapshot of the model instead of the version at the end of training. The code to do this is below:\n",
    "<pre>\n",
    "<code>\n",
    "<!-- language: lang-python -->\n",
    "torch.save(best_model_wts,\"best.ckpt\") # to save on disk for later runs\n",
    "model.load_state_dict(best_model_wts) # to load the weights of best validation score\n",
    "</code>\n",
    "</pre>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all embeddings\n",
    "embs =[]\n",
    "labs = []\n",
    "for b,l in iris_loader:\n",
    "    with torch.no_grad():\n",
    "        embs.append(model(b.to(device)).cpu())\n",
    "        labs.append(l)\n",
    "embs = torch.vstack(embs)\n",
    "labs = torch.cat(labs).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dists = torch.zeros(len(labs.unique()),len(labs.unique()))\n",
    "counts = torch.zeros(len(labs.unique()),len(labs.unique()))\n",
    "for i,e in enumerate(embs):\n",
    "    l1 = labs[i]-1\n",
    "    for j,e2 in enumerate(embs):\n",
    "        l2 = labs[j]-1\n",
    "        dists[l1,l2]+=torch.dist(e,e2)\n",
    "        counts[l1,l2]+=1\n",
    "\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "ax = plt.subplot()\n",
    "im = ax.imshow(((dists/counts).numpy()/(dists/counts).max()).numpy(),cmap = 'jet');\n",
    "divider = make_axes_locatable(ax)\n",
    "cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "plt.colorbar(im,cax=cax);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Q10: </b> Visualize the embeddings in a lower-dimensional space (2D or 3D) using a dimensionality reduction method. Interpret the plot.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we expected, the model pushed the embeddings of different subjects apart in the latent space. Now all we have to do is to enhance and encode the perp image, then we can bring them to justice!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Q11: </b> Ball's in your field. Enhance the perpetrator iris image, push it through the encoder you trained , and construct a similarity table just as you did with the fingerprints. Visualize your similarity scores and choose a threshold. Discuss results in your report.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Tip:</b> Don't forget to apply the enhancement and masking to your perpetrator iris image.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How is this possible? Iris scans don't provide a clear suspect either?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_iris_image_by_label(ind):\n",
    "    for i in range(len(iris_labels)):\n",
    "        if iris_labels[i] == ind:\n",
    "            return enhanced_iris[i]\n",
    "    raise ValueError(\"No image with this label is found.\")\n",
    "\n",
    "imgs = [get_iris_image_by_label(i) for i in sim_tb2.values[:10,0]]\n",
    "\n",
    "plot_image_sequence(imgs,kneedle.knee if kneedle.knee<=7 else 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no doubt about it now, there is an insider messing with the data! Lucky for us, but unlucky for them, they slipped up. We can now construct a multimodal biometric system in order to catch the perpetrator!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV Multimodal System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Score Fusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Q12: </b> Fuse your iris and fingerprint biometric system on the score level to make a prediction and solve the murder case! Use the metrics you implemented in the previous assignment to assess your system. Do you feel confident in your prediction? How do you fuse the scores? Why?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V. Assignment Instructions\n",
    "Both a report and the implementation in this notebook have to be submitted to toledo. <u>The notebook functions as supplementary material only!</u> The report should be self contained. Feel free to add figures, code and mathematics in the report if you feel comfortable. Try to be concice and to the point!  \n",
    "The assignment deadline is 28/04/2024, 11 PM. Make sure to zip your notebook, your report and any additional resources you use (data, open-source code, etc.) and name your .zip file as **Assignment2_[your student number].zip**.\n",
    "\n",
    "1. Follow the instructions in the notebook and discuss results/impact in your report.\n",
    "2. <b>Choose a number of tasks equivalent to <u>at least 6pts</u> from the list below (pts are not related to the grades): </b>\n",
    "    1. [FING] OpenCV provides different KeyPoint detectors and descriptors (ORB, SIFT, SURF, BRIEF, ...). Briefly test, visually, which of these seem to extract relatively reliable and interesting points from the fingerprints dataset (you can skip the ones that require a lisence) Compare them to minutiae. (1pt)\n",
    "    2. [FING and IRIS] Evaluate your biometric system in an authentication scenario, do you still aggregate the similarity scores? Why? (1pt)\n",
    "    3. [FING] Play around with the gabor filter bank, what is the effect of the parameters? Can you setup a databank that results in better features? (1pt)\n",
    "    4. [FING] Incorporate the orientations of the minutiae to your similarity scores and compare. (1pt)\n",
    "    4. [FING] Can you find a better way to aggregate the gabor feature maps? Analyse and report. (1pt) \n",
    "    5. [IRIS] Compare the performance of OpenCV Keypoint detectors to that of the triplet loss encoder in an identification setting (1pt).\n",
    "    6. [FING] Try a neural network approach in order to get similarity scores directly from image data without feature extraction. (3pt)\n",
    "    7. [FING or IRIS] Attempt to improve the segmentation, evaluate. (3pt) \n",
    "    8. [FING] Attempt to improve the feature extraction using a completely different technique. Evaluate your results. (3pt)\n",
    "    9.  [MULTI] Fuse your modalities on the feature level instead of the score level (3pt)\n",
    "    10. [ . ] Create a biometric identification or authentication system on a modality of choice and evaluate it (6pt)\n",
    "        - e.g. on smartphone usage patterns \n",
    "        - Excludes; Fingerprints, irises and faces\n",
    "    11. [ . ] Create a machine learning based identification/authentication system that will process the fingerprints and the irises simultaneously,and evaluate it (6pt)\n",
    "        - e.g. based on deep learning \n",
    "        - training is required, transfer learning is allowed\n",
    "3. All results should be discussed in detail in your report. \n",
    "\n",
    "\n",
    "<em>Note: Indicate clearly which tasks you end up choosing and where we can find the implementations and/or results. </em> <br>\n",
    "<em>Note 2: Usually there are multiple valid solutions, you should always defend your approach and verbally compare it to the other approaches (possible advantages/disadvantages) in your report! If you do not provide proper reasoning we can not give you the grades you deserve! </em>\n",
    "\n",
    "Good luck, have fun!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ecf5722fdaf1897a315d257d89d94520bfcaa453217d5becf09b39e73618b0de"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
